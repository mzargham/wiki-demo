{"url_to_unified_index": {"https://medium.com/@vineethveetil/llama-2-vs-llama-3-an-in-depth-comparison-aebb6a3f8c51": 8, "https://deasadiqbal.medium.com/technical-details-about-llama-3-7fa206134950": 7, "https://en.wikipedia.org/wiki/Llama_(language_model)": 9, "https://medium.com/towards-generative-ai/understanding-llama-2-architecture-its-ginormous-impact-on-genai-e278cb81bd5c": 6, "https://kodexolabs.com/llama-3/": 14, "https://about.fb.com/news/2024/05/how-companies-are-using-meta-llama/": 5, "https://www.mathaware.org/unlocking-the-power-of-llama-3-applications-across-industries-explained-llm-series/": 19, "https://www.linkedin.com/pulse/comprehensive-technical-analysis-llama-3-comparison-2-ibad-rehman-kw8pe": 1, "https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-meta-llama-3-models-on-azure-ai-model-catalog/ba-p/4117144": 20, "https://myscale.com/blog/llama-3-vs-phi-3-superior-ai-model-comparison/": 18, "https://www.ccn.com/news/technology/llama-3-release-date/": 21, "https://labelstud.io/blog/fine-tuning-llama-3-enhancing-accuracy-in-medical-q-and-a-with-llms/": 4, "https://medium.com/@soumava.dey.aig/decoding-llama-3-a-quick-overview-of-the-model-7e69abcdbe6a": 10, "https://sapling.ai/llm/llama2-vs-llama3": 11, "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct": 12, "https://kili-technology.com/large-language-models-llms/llama-3-guide-everything-you-need-to-know-about-meta-s-new-model-and-its-data": 3, "https://daily.dev/blog/meta-llama-3-everything-you-need-to-know-in-one-place": 2, "https://builtin.com/articles/llama-3": 15, "https://em360tech.com/tech-article/what-is-llama-3": 13, "https://mlops.community/budget-instruction-fine-tuning-of-llama-3-8b-instructon-medical-data-with-hugging-face-google-colab-and-unsloth/": 16, "https://en.wikipedia.org/wiki/Large_language_model": 17}, "url_to_info": {"https://medium.com/@vineethveetil/llama-2-vs-llama-3-an-in-depth-comparison-aebb6a3f8c51": {"url": "https://medium.com/@vineethveetil/llama-2-vs-llama-3-an-in-depth-comparison-aebb6a3f8c51", "description": "Training Data Scale Llama 3 has been pre-trained on over 15T tokens sourced from publicly available data. Its training dataset is seven times larger than that used for Llama 2 and includes four times more code. Over 5% of the Llama 3 pre-training dataset consists of high-quality, non-English ...", "snippets": ["Continued Performance Improvement Notwithstanding a much lower Chinchilla-optimal training compute corresponding to ~200B for an 8B parameter model, model performance continues to enhance even after the model is trained on significantly more data. The model shows continuous log-linear improvement after training on up to 15T tokens.", "Optimization Techniques Meta employed techniques such as data parallelization, model parallelization, and pipeline parallelization to optimize training. To maximize GPU uptime, Meta developed a new advanced training stack that automates error detection, handling, and maintenance.", "Architectural Similarities and Differences Architecturally, there is minimal difference between the two models, aside from Llama 3 supporting a larger context window. Those expecting a shift towards bigger changes like a mixture of experts (MoE) may find this disappointing. This highlights that most performance improvements are driven by enhancements in data quality and data size, and pre/post training methodologies.", "Furthermore, improvements in hardware reliability and new detection mechanisms for silent data corruption, coupled with scalable storage systems that reduce the overheads of checkpointing and rollback, have resulted in significant improvement in efficiencies in training. Architectural Similarities and Differences Architecturally, there is minimal difference between the two models, aside from Llama 3 supporting a larger context window.", "Enhancements and Improvements Meta reports that upgrades to their post-training methods and fine tuning for Llama 3 have reduced false refusal rates, enhanced alignment, and diversified model responses. These improvements have also advanced capabilities in reasoning and code generation.", "Training Data Scale Llama 3 has been pre-trained on over 15T tokens sourced from publicly available data. Its training dataset is seven times larger than that used for Llama 2 and includes four times more code. Over 5% of the Llama 3 pre-training dataset consists of high-quality, non-English data covering more than 30 languages."], "title": "Llama 2 vs Llama 3: An In-depth Comparison | by Dr. Vineeth Veetil @UMich @IIT B | Medium"}, "https://deasadiqbal.medium.com/technical-details-about-llama-3-7fa206134950": {"url": "https://deasadiqbal.medium.com/technical-details-about-llama-3-7fa206134950", "description": "The Llama 3 architecture is based on a decoder-only model and includes a new, highly optimized 128k tokenizer. This is quite notable, given that, with few exceptions, most large language models simply reuse the same tokenizers. The new tokenizer leads to major performance gains.", "snippets": ["The Llama 3 architecture is based on a decoder-only model and includes a new, highly optimized 128k tokenizer. This is quite notable, given that, with few exceptions, most large language models simply reuse the same tokenizers. The new tokenizer leads to major performance gains. Another area of improvement in the architecture is the grouped query attention, which was already used in Llama 2 but has been enhanced for the larger models."], "title": "Technical Details About Llama 3. Meta AI has introduced a new tokenizer\u2026 | by Asad iqbal | Medium"}, "https://en.wikipedia.org/wiki/Llama_(language_model)": {"url": "https://en.wikipedia.org/wiki/Llama_(language_model)", "description": "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models. The accompanying preprint also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.", "snippets": ["Tunney et. al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for FP16 and 8-bit quantized data types. Wired describes the 8B parameter version of Llama 3 as being \"surprisingly capable\" given its size.", "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models. The accompanying preprint also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.", "Llama (acronym for Large Language Model Meta AI, and formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3, released in April 2024. Model weights for the first version of Llama were ...", "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model. After the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities."], "title": "Llama (language model) - Wikipedia"}, "https://medium.com/towards-generative-ai/understanding-llama-2-architecture-its-ginormous-impact-on-genai-e278cb81bd5c": {"url": "https://medium.com/towards-generative-ai/understanding-llama-2-architecture-its-ginormous-impact-on-genai-e278cb81bd5c", "description": "It\u2019s trained on 2 Trillion tokens, ... on human evaluation \u00b7 The biggest novelty is the improvement over OpenAI architecture, on safety vs Helpfulness model with models performance not degrading as it becomes safer....", "snippets": ["The greatest thing since the sliced bread dropped last week in the form of Llama-2. Meta released it with an open license for both research & commercial purposes. A closer look at the license terms\u2026", "The most significant breakthrough introduced by LLAMA2 is overcoming the commonly observed tradeoff between safety and helpfulness, achieving superior performance on both criteria. To accomplish this, Meta trained two distinct reward models: one optimized for helpfulness, referred to as the Helpfulness RM, and another for safety, referred to as the Safety RM. The model architecture and hyper-parameters remain the same as those of the pre-trained language models, except for the classification head for next-token prediction, which is replaced with a regression head for generating the scalar reward.", "The overall outcome showcased improved attention compared to the existing model. However, it\u2019s important to note that this approach was evaluated on 70B models exclusively. Meta asked three annotators to judge the quality of the answers based on a 7-point Likert scale (the higher the better) and calculated IRR (inter-rater reliability) to ensure consistency in quality. LLAMA-2 Chat the outperform open-source models by a significant margin(60\u201375%) on both single-turn and multi-turn prompts and comparable to ChatGPT."], "title": "Understanding LLaMA-2 Architecture & its Ginormous Impact on GenAI | by Kunal Sawarkar | Towards Generative AI | Medium"}, "https://kodexolabs.com/llama-3/": {"url": "https://kodexolabs.com/llama-3/", "description": "How Llama 3 by Meta is ready & geared up for the world of possibilities & challenges in the technological advancements by large language models.", "snippets": ["While benchmarks currently suggest that Meta\u2019s Llama 3 might have an edge in efficiency for certain tasks, Google might respond with advancements of their own. Ultimately, this competition benefits everyone. As these meta large language models become more efficient and powerful, the potential applications across various fields, from scientific research to creative content generation, become even more exciting. Large language models (LLMs) are revolutionizing the way we interact with technology.", "Their technology can seamlessly integrate with other Meta products and services, creating a unified and streamlined workflow. This fosters better data exchange and collaboration within your existing infrastructure. Meta AI is heavily invested in research and development, constantly innovating, and improving its meta large language models. This ongoing dedication translates into regular updates and advancements for users."], "title": "Llama 3 | Meta AI on the Verge of Revolution"}, "https://about.fb.com/news/2024/05/how-companies-are-using-meta-llama/": {"url": "https://about.fb.com/news/2024/05/how-companies-are-using-meta-llama/", "description": "Our open-source large language AI models are benefiting organizations across industries including education, video communications, research and medicine. Companies are using Meta Llama to make education content more localized to students, summarize video calls, and provide medical information ...", "snippets": ["Our open-source large language AI models are benefiting organizations across industries including education, video communications, research and medicine. Companies are using Meta Llama to make education content more localized to students, summarize video calls, and provide medical information in low-resource settings."], "title": "How Companies Are Using Meta Llama | Meta"}, "https://www.mathaware.org/unlocking-the-power-of-llama-3-applications-across-industries-explained-llm-series/": {"url": "https://www.mathaware.org/unlocking-the-power-of-llama-3-applications-across-industries-explained-llm-series/", "description": "Unlock the power of Llama 3 across industries! Enhance financial forecasting, healthcare analytics, and retail optimization with its predictive modelling and user-friendly interface. Find the article for insights on driving data-driven decisions through machine learning algorithms and in-depth ...", "snippets": ["Unlock the power of Llama 3 across industries! Enhance financial forecasting, healthcare analytics, and retail optimization with its predictive modelling and user-friendly interface. Find the article for insights on driving data-driven decisions through machine learning algorithms and in-depth analysis."], "title": "Unlocking the Power of Llama 3: Applications Across Industries Explained LLM Series | MathAware"}, "https://www.linkedin.com/pulse/comprehensive-technical-analysis-llama-3-comparison-2-ibad-rehman-kw8pe": {"url": "https://www.linkedin.com/pulse/comprehensive-technical-analysis-llama-3-comparison-2-ibad-rehman-kw8pe", "description": "Llama 3 distinguishes itself from its predecessor, Llama 2, with a broader scope and an array of enhanced features aimed at providing more accurate and comprehensive responses. One of the critical areas of improvement is its ability to handle a wider variety of questions, including those that touch on more sensitive or controversial topics. This capability is a significant ...", "snippets": ["This approach not only accelerates innovation by enabling a broader community to contribute and build upon the Llama models but also ensures that the benefits of AI advancements are widely distributed. As we look to the future, Meta's commitment to developing models that are both powerful and user-friendly, coupled with a focus on ethical AI development, suggests a roadmap that is as much about fostering community and collaboration as it is about technological breakthroughs."], "title": "A Comprehensive Technical Analysis of Llama 3 & Comparison with Llama 2"}, "https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-meta-llama-3-models-on-azure-ai-model-catalog/ba-p/4117144": {"url": "https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-meta-llama-3-models-on-azure-ai-model-catalog/ba-p/4117144", "description": "This feature is useful for workflows created with Llama 3; it enables a comprehensive assessment using metrics such as groundedness, which gauges the pertinence and accuracy of the model's responses based on the input sources when using a retrieval augmented generation (RAG) pattern. Client integration: You can use the API and key ...", "snippets": ["Content Safety Integration: Customers can integrate Meta Llama 3 models with content safety features available through Azure AI Content Safety, enabling additional responsible AI practices. This integration facilitates the development of safer AI applications, ensuring content generated or processed is monitored for compliance and ethical standards."], "title": "Meta Llama 3 Models Launch on Azure AI: Next-Gen LLM Performance and Integration"}, "https://myscale.com/blog/llama-3-vs-phi-3-superior-ai-model-comparison/": {"url": "https://myscale.com/blog/llama-3-vs-phi-3-superior-ai-model-comparison/", "description": "Explore the comparison between Llama 3 and Phi 3 AI models, showcasing their superior capabilities and performance in the evolving technological landscape.", "snippets": ["As we navigate towards a future intertwined with advanced AI technologies, ethical dilemmas surrounding the behavior of models like Llama-3 and Phi-3 become paramount. Ensuring that these models engage with complex content ethically while avoiding harmful actions (opens new window) poses a significant challenge. Balancing technological advancements with ethical responsibilities will be crucial in harnessing the full potential of AI for societal betterment.", "Balancing technological advancements with ethical responsibilities will be crucial in harnessing the full potential of AI for societal betterment. As we reflect on the advancements brought by models like Llama 3 and Phi 3, it's essential to consider the role of efficient data management in AI development."], "title": "Llama 3 vs Phi 3: Superior AI Model Comparison"}, "https://www.ccn.com/news/technology/llama-3-release-date/": {"url": "https://www.ccn.com/news/technology/llama-3-release-date/", "description": "Meta has will release a small version of LLaMA-3 within the next month, with a larger version scheduled for later this year.", "snippets": ["Home / News / Technology / Meta\u2019s Llama 3 Release Date \u2013 What to Expect From Open-Source AI Model? ... Meta Vice President Nick Clegg has confirmed that Llama-3 will be released imminently. Photo by Riccardo Savi/Getty Images for Concordia Summit."], "title": "Meta\u2019s Llama 3 Release Date \u2013 What to Expect From Open-Source AI Model? | CCN.com"}, "https://labelstud.io/blog/fine-tuning-llama-3-enhancing-accuracy-in-medical-q-and-a-with-llms/": {"url": "https://labelstud.io/blog/fine-tuning-llama-3-enhancing-accuracy-in-medical-q-and-a-with-llms/", "description": "A flexible data labeling tool for all data types. Prepare training data for computer vision, natural language processing, speech, voice, and video models.", "snippets": ["These resources are designed to streamline the workflow and enhance the practical application of our development strategy. Data Curation Notebook: Utilizes Label Studio for data annotation and preparation. Fine-tuning Notebook: Conducts the fine-tuning processes on a Colab T4 instance, optimized for these tasks. These resources are designed to make the development workflow more efficient and to demonstrate a practical implementation of our iterative strategy. This structured, iterative development approach ensures that Llama 3 is not only adapted to medical Q&A but can facilitate continual improvement through systematic evaluation and refinement.", "The costs associated with this curation\u2014both financial and in terms of human labor\u2014are considerable, yet the needs are justified if we are to create safe, reliable medical AI systems. In this article, we want to demonstrate a method of curating large datasets to reduce the cost for curating a high quality medical Q&A dataset in Label Studio and fine-tuning Llama 3 on this data.", "When it comes to medical information, the stakes for accuracy and reliability are even higher, as misinformation can lead to real-world health risks. The costs associated with this curation\u2014both financial and in terms of human labor\u2014are considerable, yet the needs are justified if we are to create safe, reliable medical AI systems."], "title": "Fine-Tuning Llama 3: Enhancing Accuracy in Medical Q&A With LLMs | Label Studio"}, "https://medium.com/@soumava.dey.aig/decoding-llama-3-a-quick-overview-of-the-model-7e69abcdbe6a": {"url": "https://medium.com/@soumava.dey.aig/decoding-llama-3-a-quick-overview-of-the-model-7e69abcdbe6a", "description": "Generative AI has intensified as more proprietary models have been released within a span of one year for industrial and personal purposes. However, the open-source community did not benefit from\u2026", "snippets": ["Llama 3 70B even goes further by showing the best overall performance score, matching that of the most powerful proprietary models around, such as Gemini Pro 1.5 and Claude 3 Sonnet. ... - MMLU: The Massive Multitask Language Understanding, is a benchmark designed to measure an AI\u2019s ability to understand a wide range of subjects and perform tasks based on that understanding"], "title": "Decoding Llama 3: A Quick Overview of the Model | by Soumava Dey | Medium"}, "https://sapling.ai/llm/llama2-vs-llama3": {"url": "https://sapling.ai/llm/llama2-vs-llama3", "description": "Improvements to the pretraining -- 7X more data than Llama 2 --- and post-training -- careful curation of instruction-tuning data -- processes result in improved alignment and output quality ... Looking for an LLM API/SDK that works out of the box? No prompts or ad hoc guardrails.", "snippets": ["The successor to Llama 2, Llama 3 demonstrates state-of-the-art performance on benchmarks and is, according to Meta, the \"best open source models of their class, period\". Improvements to the pretraining -- 7X more data than Llama 2 --- and post-training -- careful curation of instruction-tuning data -- processes result in improved alignment and output quality"], "title": "Llama 2 vs. Llama 3: Which LLM is Better? | Sapling"}, "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct": {"url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct", "description": "Model Architecture Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Llama 3 family ...", "snippets": ["The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Llama 3 family of models. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability."], "title": "meta-llama/Meta-Llama-3-8B-Instruct \u00b7 Hugging Face"}, "https://kili-technology.com/large-language-models-llms/llama-3-guide-everything-you-need-to-know-about-meta-s-new-model-and-its-data": {"url": "https://kili-technology.com/large-language-models-llms/llama-3-guide-everything-you-need-to-know-about-meta-s-new-model-and-its-data", "description": "In April 2024, Meta released Llama 3, a large language model touted to be more powerful and diverse than its predecessors, Llama 1 and 2. The family of models has a 70b parameter and a smaller 8b parameter version, both of which have instruction-tuned versions as well.", "snippets": ["For example, we have demonstrated a simple fine-tuning process in one of our previous webinars. We used 800 data points to fine-tune the small Llama 2 7B base model for an AI agent that assists with financial analysis. We wanted our agent to provide consistent, correct answers with a specific, straightforward tone and message format. With the 800 data points, we found that it consistently provided us with the answers we needed. If we were to train it for more complex tasks, we may require more data depending on the task needed.", "We hope this article has been helpful and provided you with key insights into the data side of training, fine-tuning, and leveraging a large language model like Llama 3. The organizations we're working with are in different stages of their processes, but eliminating the bottleneck of building and managing the quality of a dataset, whether for training, fine-tuning, evaluating, or monitoring, is the first and one of the most significant steps in developing a genuinely impactful AI application. Creating a proprietary dataset for any use cases we've shared and discussed can be daunting, even for the most established data science teams.", "Its performance is on par with other leading models in the industry, making it a versatile tool for various applications in natural language processing. Both Llama 3 8B and 70B models outperformed other open-source models like Mistral 7B and Google's Gemma 7B on standard benchmarks like MMLU, ARC, DROP, GPQA, HumanEval, GSM-8K, MATH, AGIEval, and BIG-Bench Hard.", "We hope this article has been helpful and provided you with key insights into the data side of training, fine-tuning, and leveraging a large language model like Llama 3. The organizations we're working with are in different stages of their processes, but eliminating the bottleneck of building and managing the quality of a dataset, whether for training, fine-tuning, evaluating, or monitoring, is the first and one of the most significant steps in developing a genuinely impactful AI application."], "title": "Llama 3 Guide: Everything You Need to Know About Meta's New Model and Its Data"}, "https://daily.dev/blog/meta-llama-3-everything-you-need-to-know-in-one-place": {"url": "https://daily.dev/blog/meta-llama-3-everything-you-need-to-know-in-one-place", "description": "Discover the features and capabilities of Meta's latest AI tool, Llama 3, and its role in revolutionizing language interaction. Learn about architecture, performance, applications, and future plans.", "snippets": ["When using Llama 3, it's important to keep an eye on what it creates to make sure nothing harmful or wrong gets through. Here are some ways to do this: Check content by hand before sharing what the model comes up with. Use automatic checks to catch topics or words that might be a problem.", "Use feedback to make the model better over time. Keeping content in check helps keep everyone safe and makes sure things stay appropriate. Meta has made Llama 3 work fast and smart, even when it has a lot to do. They've done things like:", "Discover the features and capabilities of Meta's latest AI tool, Llama 3, and its role in revolutionizing language interaction. Learn about architecture, performance, applications, and future plans.", "This will help Llama models help people across the world by creating content, translating, and understanding stuff in many languages, making it a really helpful global assistant. Llama 3 can think about and understand text that's up to 2,048 words long. But, Meta wants to push this even further, so future versions can think about much longer texts, like whole research papers. This will allow the models to understand complicated ideas, argue points, and think deeply about big topics."], "title": "Meta Llama 3: Everything you need to know in one place"}, "https://builtin.com/articles/llama-3": {"url": "https://builtin.com/articles/llama-3", "description": "This article explains what is new about Meta AI\u2019s large language model, Llama 3. The article explains improvements in the model, which is now open source.", "snippets": ["In partnership with Qualcomm, Llama 3 is optimized for Snapdragon platforms, enhancing mobile experiences with on-device learning and direct content generation capabilities and making advanced AI features more accessible on mobile devices. Broad industry applications."], "title": "What You Need to Know about Meta AI\u2019s Llama 3 | Built In"}, "https://em360tech.com/tech-article/what-is-llama-3": {"url": "https://em360tech.com/tech-article/what-is-llama-3", "description": "Compared to previous versions like Llama 2, Llama 3 boasts better reasoning abilities, code generation, and can follow instructions more effectively. It also outperforms other open models on benchmarks that measure language understanding and response (ARC, DROP and MMLU).", "snippets": ["Llama 3 has been trained on a massive dataset of text and code, including creative writing examples. This allows it to understand the patterns and structures of different text formats. When you provide a prompt or starting point, Llama 3 can use its knowledge to generate text that adheres to the format and style you specify."], "title": "What is Meta's Llama 3? Everything you Need to Know | Enterprise Tech News EM360Tech"}, "https://mlops.community/budget-instruction-fine-tuning-of-llama-3-8b-instructon-medical-data-with-hugging-face-google-colab-and-unsloth/": {"url": "https://mlops.community/budget-instruction-fine-tuning-of-llama-3-8b-instructon-medical-data-with-hugging-face-google-colab-and-unsloth/", "description": "The MLOps Community fills the swiftly growing need to share real-world Machine Learning Operations best practices from engineers in the field.", "snippets": ["The model undergoes fine-tuning using a dataset comprising text specific to the target domain, thereby enhancing its contextual grasp and proficiency in domain-specific tasks. For example, to develop a chatbot for a medical application, the model would be trained on medical records to refine its language comprehension abilities within the healthcare domain.", "In conclusion, the process of fine-tuning LLMs stands as a crucial step towards harnessing the full potential of pre-trained models for specific tasks and domains. Despite the challenges posed by computational resources, solutions such as Google Colab\u2019s free-tier GPUs and memory management tools like Unsloth pave the way for enthusiasts to engage in this transformative process without financial barriers."], "title": "Budget Instruction Fine-tuning of Llama 3 8B Instruct(on Medical Data) with Hugging Face, Google Colab and Unsloth - MLOps Community"}, "https://en.wikipedia.org/wiki/Large_language_model": {"url": "https://en.wikipedia.org/wiki/Large_language_model", "description": "The largest and most capable LLMs, ... of large-scale text data. Historically, up to 2020, fine-tuning was the primary method used to adapt a model for specific tasks. However, larger models such as GPT-3 have demonstrated the ability to achieve similar results through prompt engineering, which involves crafting specific input prompts to guide the model's responses. These models acquire knowledge about syntax, semantics, and ontologies inherent in human language corpora, but ...", "snippets": ["Because language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models' training data inadvertently includes portions of any given test set."], "title": "Large language model - Wikipedia"}}}