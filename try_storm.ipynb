{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs\n",
    "from knowledge_storm.lm import OpenAIModel\n",
    "from knowledge_storm.rm import YouRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_configs = STORMWikiLMConfigs()\n",
    "openai_kwargs = {\n",
    "    'api_key': os.getenv(\"OPENAI_API_KEY\"),\n",
    "    'temperature': 1.0,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n",
    "# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n",
    "# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\n",
    "gpt_35 = OpenAIModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)\n",
    "gpt_4 = OpenAIModel(model='gpt-4o', max_tokens=3000, **openai_kwargs)\n",
    "lm_configs.set_conv_simulator_lm(gpt_4)\n",
    "lm_configs.set_question_asker_lm(gpt_4)\n",
    "lm_configs.set_outline_gen_lm(gpt_4)\n",
    "lm_configs.set_article_gen_lm(gpt_4)\n",
    "lm_configs.set_article_polish_lm(gpt_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_simulator_lm',\n",
       "              {'temperature': 1.0,\n",
       "               'max_tokens': 3000,\n",
       "               'top_p': 0.9,\n",
       "               'frequency_penalty': 0,\n",
       "               'presence_penalty': 0,\n",
       "               'n': 1,\n",
       "               'model': 'gpt-4o'}),\n",
       "             ('question_asker_lm',\n",
       "              {'temperature': 1.0,\n",
       "               'max_tokens': 3000,\n",
       "               'top_p': 0.9,\n",
       "               'frequency_penalty': 0,\n",
       "               'presence_penalty': 0,\n",
       "               'n': 1,\n",
       "               'model': 'gpt-4o'}),\n",
       "             ('outline_gen_lm',\n",
       "              {'temperature': 1.0,\n",
       "               'max_tokens': 3000,\n",
       "               'top_p': 0.9,\n",
       "               'frequency_penalty': 0,\n",
       "               'presence_penalty': 0,\n",
       "               'n': 1,\n",
       "               'model': 'gpt-4o'}),\n",
       "             ('article_gen_lm',\n",
       "              {'temperature': 1.0,\n",
       "               'max_tokens': 3000,\n",
       "               'top_p': 0.9,\n",
       "               'frequency_penalty': 0,\n",
       "               'presence_penalty': 0,\n",
       "               'n': 1,\n",
       "               'model': 'gpt-4o'}),\n",
       "             ('article_polish_lm',\n",
       "              {'temperature': 1.0,\n",
       "               'max_tokens': 3000,\n",
       "               'top_p': 0.9,\n",
       "               'frequency_penalty': 0,\n",
       "               'presence_penalty': 0,\n",
       "               'n': 1,\n",
       "               'model': 'gpt-4o'})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_configs.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the STORMWikiRunnerArguments class for more configurations.\n",
    "engine_args = STORMWikiRunnerArguments(\"outputs\")\n",
    "rm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)\n",
    "runner = STORMWikiRunner(engine_args, lm_configs, rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Llama 3 (Language Model)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "knowledge_storm.interface : INFO     : run_knowledge_curation_module executed in 133.1669 seconds\n",
      "knowledge_storm.interface : INFO     : run_outline_generation_module executed in 7.1936 seconds\n",
      "sentence_transformers.SentenceTransformer : INFO     : Use pytorch device_name: mps\n",
      "sentence_transformers.SentenceTransformer : INFO     : Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2\n",
      "knowledge_storm.interface : INFO     : run_article_generation_module executed in 16.1945 seconds\n",
      "knowledge_storm.interface : INFO     : run_article_polishing_module executed in 237.9508 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Execution time *****\n",
      "run_knowledge_curation_module: 133.1669 seconds\n",
      "run_outline_generation_module: 7.1936 seconds\n",
      "run_article_generation_module: 16.1945 seconds\n",
      "run_article_polishing_module: 237.9508 seconds\n",
      "***** Token usage of language models: *****\n",
      "run_knowledge_curation_module\n",
      "    gpt-4o: {'prompt_tokens': 29876, 'completion_tokens': 18213}\n",
      "run_outline_generation_module\n",
      "    gpt-4o: {'prompt_tokens': 489, 'completion_tokens': 407}\n",
      "run_article_generation_module\n",
      "    gpt-4o: {'prompt_tokens': 11345, 'completion_tokens': 3271}\n",
      "run_article_polishing_module\n",
      "    gpt-4o: {'prompt_tokens': 3184, 'completion_tokens': 475}\n",
      "***** Number of queries of retrieval models: *****\n",
      "run_knowledge_curation_module: {'YouRM': 36}\n",
      "run_outline_generation_module: {'YouRM': 0}\n",
      "run_article_generation_module: {'YouRM': 0}\n",
      "run_article_polishing_module: {'YouRM': 0}\n"
     ]
    }
   ],
   "source": [
    "runner.run(\n",
    "    topic=topic,\n",
    "    do_research=True,\n",
    "    do_generate_outline=True,\n",
    "    do_generate_article=True,\n",
    "    do_polish_article=True,\n",
    ")\n",
    "runner.post_run()\n",
    "runner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
