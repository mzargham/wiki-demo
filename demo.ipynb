{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import re\n",
    "dir = os.getcwd()\n",
    "os.chdir(\"../profbot\")\n",
    "from exam import Exam, GradeLog\n",
    "from llmtest import Llm, LLMTest\n",
    "os.chdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'poke, please reply'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_llm = Llm(model_identifier=\"gpt-4o\")\n",
    "eval_llm.prompt(\"poke, please reply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes from the CSV files\n",
    "rdf = pd.read_csv(\"evaluators/requirements_data/section_level_requirements.csv\")\n",
    "stdf = pd.read_csv(\"evaluators/requirements_data/section_types.csv\")\n",
    "rbsdf = pd.read_csv(\"evaluators/requirements_data/requirements_by_section_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead section</td>\n",
       "      <td>A concise summary of the article, never divide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Body sections</td>\n",
       "      <td>Main content of the article, divided into logi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Infobox</td>\n",
       "      <td>Right-aligned summary of key facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>References</td>\n",
       "      <td>Section listing the sources cited in the article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>See also</td>\n",
       "      <td>Internal links to related English Wikipedia ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Further reading</td>\n",
       "      <td>Relevant books, articles, or other publication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>External links</td>\n",
       "      <td>Relevant and appropriate websites not used as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Categories</td>\n",
       "      <td>Navigational boxes and categories at the end o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Notes</td>\n",
       "      <td>Additional information and explanations not pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                                        description\n",
       "0     Lead section  A concise summary of the article, never divide...\n",
       "1    Body sections  Main content of the article, divided into logi...\n",
       "2          Infobox                 Right-aligned summary of key facts\n",
       "3       References   Section listing the sources cited in the article\n",
       "4         See also  Internal links to related English Wikipedia ar...\n",
       "5  Further reading  Relevant books, articles, or other publication...\n",
       "6   External links  Relevant and appropriate websites not used as ...\n",
       "7       Categories  Navigational boxes and categories at the end o...\n",
       "8            Notes  Additional information and explanations not pa..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"outputs/Llama_3_(Language_Model)/storm_gen_article_polished.txt\"\n",
    "article_name = path.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    section: str\n",
    "    section_type: str\n",
    "    subsection: str\n",
    "    paragraph_number: int\n",
    "    claim_number: int\n",
    "    text: str\n",
    "\n",
    "def identify_sections(text, level=1):\n",
    "    # Identify the sections\n",
    "    if level == 1:\n",
    "        sections= split = text.split(\"\\n# \")\n",
    "    elif level == 2:\n",
    "        sections = text.split(\"\\n## \")\n",
    "    else:\n",
    "        print(\"Invalid level\")\n",
    "\n",
    "    names = [section.split(\"\\n\")[0] for section in sections]\n",
    "    return names, sections\n",
    "\n",
    "def articulate_article(article):\n",
    "    # loop through sections\n",
    "    sentences = []\n",
    "    outline, sections = identify_sections(article)\n",
    "    for s in sections:\n",
    "        st = get_section_type(s)\n",
    "        suboutline,subsections = identify_sections(s, level=2)\n",
    "        for ss in subsections:\n",
    "            paragraphs = ss.split(\"\\n\")\n",
    "            for p in range(1,len(paragraphs)):\n",
    "                pa = paragraphs[p]\n",
    "                claims = pa.split(\". \")\n",
    "                for c in range(1,len(claims)):\n",
    "                    cl = claims[c]\n",
    "                    sentences.append(Sentence(outline[sections.index(s)],st,suboutline[subsections.index(ss)],p,c,cl))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def extract_response(response, default = \"body sections\"):\n",
    "        # Regular expression to find the first string between < and >\n",
    "    pattern = r'<(.*?)>'\n",
    "\n",
    "    # Search for the first match in the string\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    # If a match is found, return the matched string\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def get_section_type(section):\n",
    "    prompt = \"Section types are defined in \" + str(stdf.to_json()) + \"; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\n\" + section + \"\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "\n",
    "    return extract_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"r\") as file:\n",
    "        article = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\n# summary\\n\\nLlama 3, developed by Meta, is a state-of-the-art large language model that signifies a major leap forward in the realm of artificial intelligence. Building on the successes of its predecessors, Llama 3 emphasizes ethical AI practices and community involvement through an open-source development strategy. This model stands out due to its enhanced capabilities, stemming from extensive training on a massive dataset of over 15 trillion tokens, which is seven times larger than that of Llama 2. This expansive dataset includes a rich variety of languages and code, ensuring broad applicability and robust performance across diverse tasks.\\nA crucial element of Llama 3â€™s development is its commitment to safety and appropriateness in content generation. Meta employs a combination of manual content reviews, automated checks, and iterative feedback mechanisms to maintain the modelâ€™s reliability and ethical standards. These measures are vital for ensuring that Llama 3\\'s outputs are suitable for various applications, ranging from education and healthcare to content creation and translation. The model\\'s open-source nature not only accelerates innovation but also democratizes access to cutting-edge AI technology, enabling a wider community to contribute to its development and utilize its capabilities.\\nArchitecturally, Llama 3 retains a decoder-only model structure but incorporates significant enhancements such as a new 128k tokenizer and optimized matrix multiplication kernels for x86 and ARM CPUs. These improvements, coupled with advanced training techniques like data parallelization and model parallelization, contribute to Llama 3\\'s superior performance. The model excels in various benchmarks, outperforming both open-source and proprietary models in tasks like language understanding, reasoning, and code generation. The 70 billion parameter version, in particular, matches the performance of leading proprietary models, underscoring Llama 3\\'s competitive edge in the AI landscape.\\nDespite its advancements, Llama 3 is not without limitations. Issues such as overfitting, ethical concerns, and the substantial computational resources required for training remain significant challenges. Ensuring high-quality, diverse datasets for training and fine-tuning is critical to maintaining the model\\'s performance and reliability. As the AI community continues to explore and address these challenges, the future of Llama 3 looks promising with potential enhancements and broader applications on the horizon. Metaâ€™s open-source approach and commitment to ethical AI practices position Llama 3 as a pivotal model in the ongoing evolution of artificial intelligence technology.\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nDevelopment\\n\\nLlama 3 represents a significant advancement in the development of large language models, with a strong focus on community involvement and ethical AI practices. Meta, the company behind Llama 3, has adopted an open-source approach to enable a broader community to contribute to and build upon the Llama models. This strategy not only accelerates innovation but also ensures that the benefits of AI advancements are widely distributed [1].\\nA critical aspect of the development process is maintaining the safety and appropriateness of the content generated by Llama 3. To achieve this, Meta employs several methods, including manual content review, automated checks for problematic topics or words, and iterative feedback to continually improve the model [2]. These measures are essential to ensure that the model\\'s outputs remain safe and suitable for various applications.\\nThe development strategy also involves eliminating bottlenecks associated with data management. Meta focuses on building and managing high-quality datasets for training, fine-tuning, evaluation, and monitoring. This approach is vital for developing impactful AI applications and has been shared with organizations in various stages of their processes [3]. For example, Label Studio is used for data annotation and preparation, while Colab T4 instances are utilized for fine-tuning processes, demonstrating a structured and iterative development workflow [4].\\nIn addition to technical advancements, Llama 3\\'s development emphasizes practical applications across multiple industries. Organizations in education, video communications, research, and medicine are leveraging Llama 3 to create localized educational content, summarize video calls, and provide medical information in low-resource settings [5]. This wide range of applications highlights the model\\'s versatility and its potential to address real-world challenges.\\nFurthermore, Meta has introduced significant breakthroughs with Llama 2 that continue to influence Llama 3. The Llama 2 models overcame the tradeoff between safety and helpfulness by training two distinct reward models: the Helpfulness RM and the Safety RM. These models optimize for different criteria, thereby achieving superior performance in both areas. The architecture and hyper-parameters remain consistent with previous models, with adjustments made to the classification head for next-token prediction, which is replaced with a regression head for generating scalar rewards [6].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nArchitecture\\n\\nThe Llama 3 architecture is fundamentally built on a decoder-only model, incorporating a newly developed 128k tokenizer, which marks a significant upgrade given that most large language models tend to reuse existing tokenizers[7]. This new tokenizer contributes to substantial performance gains, differentiating Llama 3 from its predecessors and competitors.\\nIn terms of data scale, Llama 3 has been pre-trained on over 15 trillion tokens, sourced from publicly available data, which is seven times larger than the dataset used for Llama 2[8]. This dataset includes four times more code and over 5% high-quality, non-English data covering more than 30 languages[8]. These enhancements in data volume and diversity are central to the model\\'s improved performance.\\nOptimization techniques such as data parallelization, model parallelization, and pipeline parallelization were employed to streamline the training process[8]. Meta also developed a new advanced training stack to maximize GPU uptime, automating error detection, handling, and maintenance[8].\\nWhile architecturally similar to its predecessor, the Llama 3 model supports a larger context window, highlighting improvements primarily driven by data quality, data size, and enhanced training methodologies rather than drastic architectural changes[8]. The grouped query attention mechanism, which was present in Llama 2, has been further optimized for the larger models in Llama 3, contributing to its performance enhancements[7].\\nA notable innovation in Llama 3 includes the introduction of new optimized matrix multiplication kernels for x86 and ARM CPUs, which improve prompt evaluation performance for FP16 and 8-bit quantized data types[9]. Despite the foundational architecture remaining largely unchanged from LLaMA-1 models, the use of 40% more data for training underpins the significant advancements observed in Llama 3[9].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nPerformance\\n\\nLlama 3 demonstrates exceptional performance across various benchmarks, solidifying its status as a leading language model in the industry. It shows continuous log-linear improvement after training on up to 15 trillion tokens, surpassing the performance of models trained on significantly less data, such as an 8 billion parameter model with a Chinchilla-optimal training compute corresponding to approximately 200 billion tokens[8].\\nThe 8 billion (8B) and 70 billion (70B) parameter versions of Llama 3 outperform other open-source models like Mistral 7B and Google\\'s Gemma 7B on standard benchmarks including MMLU (Massive Multitask Language Understanding), ARC, DROP, GPQA, HumanEval, GSM-8K, MATH, AGIEval, and BIG-Bench Hard[3]. Llama 3\\'s 70B model, in particular, showcases the best overall performance score, matching that of the most powerful proprietary models such as Gemini Pro 1.5 and Claude 3 Sonnet[10].\\nThe superior performance of Llama 3 can be attributed to several key factors. First, the pretraining process involved utilizing seven times more data than its predecessor, Llama 2, and careful curation of instruction-tuning data to enhance alignment and output quality[11]. Additionally, both the 8B and 70B models employ Grouped-Query Attention (GQA) for improved inference scalability, further contributing to their robust performance[12].\\nWhen it comes to user experience, Llama 3 has also shown improved attention mechanisms compared to previous models. Evaluations conducted exclusively on 70B models revealed that the Llama 3 Chat model outperforms other open-source models by a significant margin (60â€“75%) on both single-turn and multi-turn prompts, and is comparable to ChatGPT[6].\\nThe architecture of Llama 3 remains largely unchanged from LLaMA-1 models, but the inclusion of 40% more training data for the foundational models has also played a critical role in enhancing performance. Future releases may include a 34B parameter model upon meeting safety targets[9].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nApplications\\n\\nLlama 3 is a versatile language model with a broad range of applications across various domains. Its advanced capabilities in text generation, conversation, summarization, and translation make it a valuable tool for numerous industries.\\n\\n## Content Creation and Translation\\n\\nOne of the primary applications of Llama 3 is content creation. The model\\'s ability to generate coherent and contextually appropriate text makes it suitable for tasks such as writing articles, crafting stories, and creating marketing content. Additionally, Llama 3 excels in translation, offering robust support for multiple languages, which facilitates global communication and content localization[2][13].\\n\\n## Integration with Meta Products\\n\\nLlama 3 seamlessly integrates with other Meta products and services, streamlining workflows and enhancing data exchange within existing infrastructures[14]. This integration capability makes it a useful component in a unified and collaborative environment, improving efficiency and productivity.\\n\\n## Mobile Experience Enhancement\\n\\nThrough a partnership with Qualcomm, Llama 3 is optimized for Snapdragon platforms, thereby enhancing mobile experiences with on-device learning and direct content generation capabilities. This optimization makes advanced AI features more accessible on mobile devices, broadening the scope of Llama 3\\'s applications[15].\\n\\n## Industry-Specific Solutions\\n\\nLlama 3\\'s adaptability extends to specific industry needs. For example, in the healthcare sector, the model can be fine-tuned using domain-specific datasets, such as medical records, to enhance its proficiency in medical Q&A applications[16]. This adaptability ensures that Llama 3 can provide accurate and contextually relevant information across various professional fields.\\n\\n## Educational and Research Support\\n\\nOrganizations are leveraging Llama 3 to make educational content more localized and accessible to students. Additionally, the model is used to summarize video calls, making it a useful tool for educational and research purposes[5]. The capability to understand and process extensive text up to 2,048 words long allows Llama 3 to handle complex ideas and arguments, facilitating deeper insights and understanding[2].\\n\\n## Open Source and Community Contributions\\n\\nLlama 3\\'s open-source nature enables a broader community to contribute to and build upon its capabilities. This approach accelerates innovation and ensures that the benefits of AI advancements are widely distributed. Meta\\'s commitment to ethical AI development and fostering community collaboration suggests a future roadmap focused on both technological breakthroughs and user-friendliness[1].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nLimitations\\n\\nDespite its numerous advancements, Llama 3 faces several limitations that need to be addressed to enhance its applicability and reliability.\\n\\n## Overfitting and Data Dependence\\n\\nOne significant issue with Llama 3, as with many large language models, is the risk of overfitting to its training data. This occurs when the model performs exceptionally well on training data but fails to generalize effectively to unseen data, potentially limiting its robustness and applicability across diverse tasks and datasets[17].\\n\\n## Ethical and Reliability Concerns\\n\\nThe ethical implications of deploying Llama 3 in real-world scenarios remain a critical concern. Ensuring that the model engages with content ethically and avoids harmful actions is a substantial challenge. Balancing technological advancements with ethical responsibilities is crucial for harnessing AI\\'s full potential for societal betterment[18]. Furthermore, in fields like medical information, the stakes for accuracy and reliability are even higher, as misinformation can lead to significant real-world health risks[4].\\n\\n## Computational Resources and Efficiency\\n\\nTraining and fine-tuning large language models like Llama 3 demand substantial computational resources. Although improvements in hardware reliability and scalable storage systems have enhanced training efficiencies, the computational cost remains a barrier for widespread adoption and experimentation by smaller organizations and individual researchers[8][16].\\n\\n## Dataset Quality and Curation\\n\\nThe quality of the training and fine-tuning datasets is pivotal for the model\\'s performance. While Llama 3 has been trained on a significantly larger dataset compared to its predecessors, the curation of this data poses a substantial challenge. Ensuring that the data is diverse and representative without including redundant or low-quality information is essential for maintaining and improving the model\\'s performance[3]. Missteps in data curation could lead to biased or incorrect model outputs, which are especially problematic in sensitive applications like medical Q&A[4].\\n\\n## Evaluation Challenges\\n\\nEvaluating large language models accurately is challenging due to the massive datasets involved and the nuanced nature of human language. Traditional metrics like perplexity might not fully capture a model\\'s performance, especially as the risk of inadvertently including portions of test sets in the training data increases with larger corpora[17].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nReception\\n\\nThe reception of Llama 3, Meta\\'s latest language model, has been overwhelmingly positive across various sectors. The model has been praised for its significant advancements in efficiency and capability over its predecessors. Llama 3 has demonstrated improved attention mechanisms and alignment, resulting in a more nuanced understanding and generation of responses. Enhancements in reasoning and code generation are also notable, contributing to a diverse range of model responses and reduced false refusal rates [8].\\nIn the AI research community, Llama 3 has outperformed open-source models by a significant margin, particularly in single-turn and multi-turn prompts, showing comparable results to ChatGPT [6]. This performance has been bolstered by rigorous evaluation methods, including the use of a 7-point Likert scale for answer quality assessment and the calculation of inter-rater reliability (IRR) to ensure consistency [6].\\nIndustries ranging from education to healthcare have leveraged Llama 3 for various applications. For instance, the model has been used to localize educational content for students, summarize video communications, and provide medical information in low-resource settings [5]. The flexibility and power of Llama 3 have also been highlighted in its use for enhancing financial forecasting, healthcare analytics, and retail optimization through predictive modeling and user-friendly interfaces [19].\\nThe release of Llama 3 has sparked a competitive atmosphere in the AI landscape, with other tech giants like Google potentially poised to respond with their advancements. This competition is viewed as beneficial for the broader AI community, driving innovation and efficiency across the field [14].\\nAdditionally, Meta\\'s commitment to open-source development and ethical AI practices has been a significant factor in the positive reception of Llama 3. By making the model available under an open license, Meta has enabled a broader community to contribute and build upon its advancements, ensuring that the benefits of AI are widely distributed [6][1]. Integrations with content safety features, such as those available through Azure AI Content Safety, further ensure that the model\\'s applications adhere to responsible AI practices [20].\\n\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\nFuture Prospects\\n\\nThe future of Llama 3 looks promising with several exciting developments on the horizon. One of the key aspects of Llama 3\\'s future is its open-source nature, which not only accelerates innovation by enabling a broader community to contribute and build upon the models but also ensures that the benefits of AI advancements are widely distributed[1]. This open-source strategy reflects Meta\\'s commitment to fostering community and collaboration, which is as crucial as the technological breakthroughs themselves.\\nAs we advance, ethical considerations surrounding the behavior of models like Llama 3 will become increasingly paramount. Ensuring these models engage with complex content ethically while avoiding harmful actions is a significant challenge[18]. Balancing these ethical responsibilities with technological advancements will be crucial in harnessing the full potential of AI for societal betterment. Efficient data management will also play a critical role in this regard, as it is essential for training, fine-tuning, and leveraging large language models effectively[18].\\nThe anticipated release of Llama 3 has generated considerable excitement, particularly with Meta Vice President Nick Clegg confirming its imminent launch[21]. Alongside Llama 3, Meta is integrating virtual assistant features into platforms like Facebook and WhatsApp, which are based on the Llama 3 model[9]. This integration exemplifies the broader applicability and utility of Llama 3 across various services and platforms.\\nThe development strategy for Llama 3 includes the use of resources such as the Data Curation Notebook and the Fine-tuning Notebook, designed to streamline the workflow and enhance the practical application of the model[4]. This structured, iterative development approach ensures continuous improvement and adaptation of Llama 3 for specific use cases, such as medical Q&A, through systematic evaluation and refinement.\\nMoreover, Llama 3 has shown exceptional performance, particularly the 70B model, which matches the performance of some of the most powerful proprietary models like Gemini Pro 1.5 and Claude 3 Sonnet[10]. This superior performance on benchmarks like the Massive Multitask Language Understanding (MMLU) highlights Llama 3\\'s capability to understand and perform a wide range of tasks effectively.\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n"
     ]
    }
   ],
   "source": [
    "data = articulate_article(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>section_type</th>\n",
       "      <th>subsection</th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>claim_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Building on the successes of its predecessors,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This model stands out due to its enhanced capa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This expansive dataset includes a rich variety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Meta employs a combination of manual content r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>These measures are vital for ensuring that Lla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Efficient data management will also play a cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Alongside Llama 3, Meta is integrating virtual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>This integration exemplifies the broader appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>This structured, iterative development approac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>This superior performance on benchmarks like t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             section   section_type        subsection  paragraph_number  \\\n",
       "0          # summary   Lead section         # summary                 2   \n",
       "1          # summary   Lead section         # summary                 2   \n",
       "2          # summary   Lead section         # summary                 2   \n",
       "3          # summary   Lead section         # summary                 3   \n",
       "4          # summary   Lead section         # summary                 3   \n",
       "..               ...            ...               ...               ...   \n",
       "70  Future Prospects  body sections  Future Prospects                 3   \n",
       "71  Future Prospects  body sections  Future Prospects                 4   \n",
       "72  Future Prospects  body sections  Future Prospects                 4   \n",
       "73  Future Prospects  body sections  Future Prospects                 5   \n",
       "74  Future Prospects  body sections  Future Prospects                 6   \n",
       "\n",
       "    claim_number                                               text  \n",
       "0              1  Building on the successes of its predecessors,...  \n",
       "1              2  This model stands out due to its enhanced capa...  \n",
       "2              3  This expansive dataset includes a rich variety...  \n",
       "3              1  Meta employs a combination of manual content r...  \n",
       "4              2  These measures are vital for ensuring that Lla...  \n",
       "..           ...                                                ...  \n",
       "70             3  Efficient data management will also play a cri...  \n",
       "71             1  Alongside Llama 3, Meta is integrating virtual...  \n",
       "72             2  This integration exemplifies the broader appli...  \n",
       "73             1  This structured, iterative development approac...  \n",
       "74             1  This superior performance on benchmarks like t...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Section types are defined in {\"name\":{\"0\":\"Lead section\",\"1\":\"Body sections\",\"2\":\"Infobox\",\"3\":\"References\",\"4\":\"See also\",\"5\":\"Further reading\",\"6\":\"External links\",\"7\":\"Categories\",\"8\":\"Notes\"},\"description\":{\"0\":\"A concise summary of the article, never divided into sections\",\"1\":\"Main content of the article, divided into logical sections\",\"2\":\"Right-aligned summary of key facts\",\"3\":\"Section listing the sources cited in the article\",\"4\":\"Internal links to related English Wikipedia articles\",\"5\":\"Relevant books, articles, or other publications not used as sources\",\"6\":\"Relevant and appropriate websites not used as sources\",\"7\":\"Navigational boxes and categories at the end of the article\",\"8\":\"Additional information and explanations not part of the main text\"}}; please assign a section type (eg Lead section or Body Sections) for the following section: \\n\\n# summary\\n\\nPlease return the name of the section type as defined in the above file but delimited in the format: <section type>'}]}\n"
     ]
    }
   ],
   "source": [
    "result = get_section_type(df.section[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lead section'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exam_question(sentence):\n",
    "    prompt = \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\n\" +sentence.text + \"\\n\\nPlease return the question in the format: <question>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "    return extract_response(response)\n",
    "\n",
    "def make_question_guidelines(sentence):\n",
    "    prompt = \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\n\" +sentence.text + \"\\n\\nPlease return the question in the format: <question>\"\n",
    "    response = eval_llm.prompt(prompt)\n",
    "    return extract_response(response)\n",
    "\n",
    "exam_guidelines = [\"evaluate according the to the requirements in the wikikpedia style guide\", \n",
    "                   \"provide a score between zero and one representing the probability this sentence meets the wikipedia guidelines according to a human wikipedia moderator\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBuilding on the successes of its predecessors, Llama 3 emphasizes ethical AI practices and community involvement through an open-source development strategy\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis model stands out due to its enhanced capabilities, stemming from extensive training on a massive dataset of over 15 trillion tokens, which is seven times larger than that of Llama 2\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis expansive dataset includes a rich variety of languages and code, ensuring broad applicability and robust performance across diverse tasks.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta employs a combination of manual content reviews, automated checks, and iterative feedback mechanisms to maintain the modelâ€™s reliability and ethical standards\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese measures are vital for ensuring that Llama 3's outputs are suitable for various applications, ranging from education and healthcare to content creation and translation\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model's open-source nature not only accelerates innovation but also democratizes access to cutting-edge AI technology, enabling a wider community to contribute to its development and utilize its capabilities.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese improvements, coupled with advanced training techniques like data parallelization and model parallelization, contribute to Llama 3's superior performance\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model excels in various benchmarks, outperforming both open-source and proprietary models in tasks like language understanding, reasoning, and code generation\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe 70 billion parameter version, in particular, matches the performance of leading proprietary models, underscoring Llama 3's competitive edge in the AI landscape.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIssues such as overfitting, ethical concerns, and the substantial computational resources required for training remain significant challenges\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring high-quality, diverse datasets for training and fine-tuning is critical to maintaining the model's performance and reliability\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAs the AI community continues to explore and address these challenges, the future of Llama 3 looks promising with potential enhancements and broader applications on the horizon\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMetaâ€™s open-source approach and commitment to ethical AI practices position Llama 3 as a pivotal model in the ongoing evolution of artificial intelligence technology.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta, the company behind Llama 3, has adopted an open-source approach to enable a broader community to contribute to and build upon the Llama models\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis strategy not only accelerates innovation but also ensures that the benefits of AI advancements are widely distributed [1].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nTo achieve this, Meta employs several methods, including manual content review, automated checks for problematic topics or words, and iterative feedback to continually improve the model [2]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese measures are essential to ensure that the model's outputs remain safe and suitable for various applications.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta focuses on building and managing high-quality datasets for training, fine-tuning, evaluation, and monitoring\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis approach is vital for developing impactful AI applications and has been shared with organizations in various stages of their processes [3]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor example, Label Studio is used for data annotation and preparation, while Colab T4 instances are utilized for fine-tuning processes, demonstrating a structured and iterative development workflow [4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nOrganizations in education, video communications, research, and medicine are leveraging Llama 3 to create localized educational content, summarize video calls, and provide medical information in low-resource settings [5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis wide range of applications highlights the model's versatility and its potential to address real-world challenges.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe Llama 2 models overcame the tradeoff between safety and helpfulness by training two distinct reward models: the Helpfulness RM and the Safety RM\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese models optimize for different criteria, thereby achieving superior performance in both areas\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe architecture and hyper-parameters remain consistent with previous models, with adjustments made to the classification head for next-token prediction, which is replaced with a regression head for generating scalar rewards [6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis new tokenizer contributes to substantial performance gains, differentiating Llama 3 from its predecessors and competitors.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis dataset includes four times more code and over 5% high-quality, non-English data covering more than 30 languages[8]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese enhancements in data volume and diversity are central to the model's improved performance.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta also developed a new advanced training stack to maximize GPU uptime, automating error detection, handling, and maintenance[8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe grouped query attention mechanism, which was present in Llama 2, has been further optimized for the larger models in Llama 3, contributing to its performance enhancements[7].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nDespite the foundational architecture remaining largely unchanged from LLaMA-1 models, the use of 40% more data for training underpins the significant advancements observed in Llama 3[9].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIt shows continuous log-linear improvement after training on up to 15 trillion tokens, surpassing the performance of models trained on significantly less data, such as an 8 billion parameter model with a Chinchilla-optimal training compute corresponding to approximately 200 billion tokens[8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nLlama 3's 70B model, in particular, showcases the best overall performance score, matching that of the most powerful proprietary models such as Gemini Pro 1.5 and Claude 3 Sonnet[10].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFirst, the pretraining process involved utilizing seven times more data than its predecessor, Llama 2, and careful curation of instruction-tuning data to enhance alignment and output quality[11]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, both the 8B and 70B models employ Grouped-Query Attention (GQA) for improved inference scalability, further contributing to their robust performance[12].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEvaluations conducted exclusively on 70B models revealed that the Llama 3 Chat model outperforms other open-source models by a significant margin (60â€“75%) on both single-turn and multi-turn prompts, and is comparable to ChatGPT[6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFuture releases may include a 34B parameter model upon meeting safety targets[9].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIts advanced capabilities in text generation, conversation, summarization, and translation make it a valuable tool for numerous industries.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model's ability to generate coherent and contextually appropriate text makes it suitable for tasks such as writing articles, crafting stories, and creating marketing content\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, Llama 3 excels in translation, offering robust support for multiple languages, which facilitates global communication and content localization[2][13].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis integration capability makes it a useful component in a unified and collaborative environment, improving efficiency and productivity.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis optimization makes advanced AI features more accessible on mobile devices, broadening the scope of Llama 3's applications[15].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor example, in the healthcare sector, the model can be fine-tuned using domain-specific datasets, such as medical records, to enhance its proficiency in medical Q&A applications[16]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis adaptability ensures that Llama 3 can provide accurate and contextually relevant information across various professional fields.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, the model is used to summarize video calls, making it a useful tool for educational and research purposes[5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe capability to understand and process extensive text up to 2,048 words long allows Llama 3 to handle complex ideas and arguments, facilitating deeper insights and understanding[2].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis approach accelerates innovation and ensures that the benefits of AI advancements are widely distributed\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta's commitment to ethical AI development and fostering community collaboration suggests a future roadmap focused on both technological breakthroughs and user-friendliness[1].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis occurs when the model performs exceptionally well on training data but fails to generalize effectively to unseen data, potentially limiting its robustness and applicability across diverse tasks and datasets[17].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring that the model engages with content ethically and avoids harmful actions is a substantial challenge\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBalancing technological advancements with ethical responsibilities is crucial for harnessing AI's full potential for societal betterment[18]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFurthermore, in fields like medical information, the stakes for accuracy and reliability are even higher, as misinformation can lead to significant real-world health risks[4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAlthough improvements in hardware reliability and scalable storage systems have enhanced training efficiencies, the computational cost remains a barrier for widespread adoption and experimentation by smaller organizations and individual researchers[8][16].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nWhile Llama 3 has been trained on a significantly larger dataset compared to its predecessors, the curation of this data poses a substantial challenge\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring that the data is diverse and representative without including redundant or low-quality information is essential for maintaining and improving the model's performance[3]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMissteps in data curation could lead to biased or incorrect model outputs, which are especially problematic in sensitive applications like medical Q&A[4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nTraditional metrics like perplexity might not fully capture a model's performance, especially as the risk of inadvertently including portions of test sets in the training data increases with larger corpora[17].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model has been praised for its significant advancements in efficiency and capability over its predecessors\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nLlama 3 has demonstrated improved attention mechanisms and alignment, resulting in a more nuanced understanding and generation of responses\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnhancements in reasoning and code generation are also notable, contributing to a diverse range of model responses and reduced false refusal rates [8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis performance has been bolstered by rigorous evaluation methods, including the use of a 7-point Likert scale for answer quality assessment and the calculation of inter-rater reliability (IRR) to ensure consistency [6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor instance, the model has been used to localize educational content for students, summarize video communications, and provide medical information in low-resource settings [5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe flexibility and power of Llama 3 have also been highlighted in its use for enhancing financial forecasting, healthcare analytics, and retail optimization through predictive modeling and user-friendly interfaces [19].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis competition is viewed as beneficial for the broader AI community, driving innovation and efficiency across the field [14].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBy making the model available under an open license, Meta has enabled a broader community to contribute and build upon its advancements, ensuring that the benefits of AI are widely distributed [6][1]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIntegrations with content safety features, such as those available through Azure AI Content Safety, further ensure that the model's applications adhere to responsible AI practices [20].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nOne of the key aspects of Llama 3's future is its open-source nature, which not only accelerates innovation by enabling a broader community to contribute and build upon the models but also ensures that the benefits of AI advancements are widely distributed[1]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis open-source strategy reflects Meta's commitment to fostering community and collaboration, which is as crucial as the technological breakthroughs themselves.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring these models engage with complex content ethically while avoiding harmful actions is a significant challenge[18]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBalancing these ethical responsibilities with technological advancements will be crucial in harnessing the full potential of AI for societal betterment\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEfficient data management will also play a critical role in this regard, as it is essential for training, fine-tuning, and leveraging large language models effectively[18].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAlongside Llama 3, Meta is integrating virtual assistant features into platforms like Facebook and WhatsApp, which are based on the Llama 3 model[9]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis integration exemplifies the broader applicability and utility of Llama 3 across various services and platforms.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis structured, iterative development approach ensures continuous improvement and adaptation of Llama 3 for specific use cases, such as medical Q&A, through systematic evaluation and refinement.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis superior performance on benchmarks like the Massive Multitask Language Understanding (MMLU) highlights Llama 3's capability to understand and perform a wide range of tasks effectively.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBuilding on the successes of its predecessors, Llama 3 emphasizes ethical AI practices and community involvement through an open-source development strategy\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis model stands out due to its enhanced capabilities, stemming from extensive training on a massive dataset of over 15 trillion tokens, which is seven times larger than that of Llama 2\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis expansive dataset includes a rich variety of languages and code, ensuring broad applicability and robust performance across diverse tasks.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta employs a combination of manual content reviews, automated checks, and iterative feedback mechanisms to maintain the modelâ€™s reliability and ethical standards\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese measures are vital for ensuring that Llama 3's outputs are suitable for various applications, ranging from education and healthcare to content creation and translation\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model's open-source nature not only accelerates innovation but also democratizes access to cutting-edge AI technology, enabling a wider community to contribute to its development and utilize its capabilities.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese improvements, coupled with advanced training techniques like data parallelization and model parallelization, contribute to Llama 3's superior performance\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model excels in various benchmarks, outperforming both open-source and proprietary models in tasks like language understanding, reasoning, and code generation\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe 70 billion parameter version, in particular, matches the performance of leading proprietary models, underscoring Llama 3's competitive edge in the AI landscape.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIssues such as overfitting, ethical concerns, and the substantial computational resources required for training remain significant challenges\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring high-quality, diverse datasets for training and fine-tuning is critical to maintaining the model's performance and reliability\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAs the AI community continues to explore and address these challenges, the future of Llama 3 looks promising with potential enhancements and broader applications on the horizon\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMetaâ€™s open-source approach and commitment to ethical AI practices position Llama 3 as a pivotal model in the ongoing evolution of artificial intelligence technology.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta, the company behind Llama 3, has adopted an open-source approach to enable a broader community to contribute to and build upon the Llama models\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis strategy not only accelerates innovation but also ensures that the benefits of AI advancements are widely distributed [1].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nTo achieve this, Meta employs several methods, including manual content review, automated checks for problematic topics or words, and iterative feedback to continually improve the model [2]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese measures are essential to ensure that the model's outputs remain safe and suitable for various applications.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta focuses on building and managing high-quality datasets for training, fine-tuning, evaluation, and monitoring\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis approach is vital for developing impactful AI applications and has been shared with organizations in various stages of their processes [3]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor example, Label Studio is used for data annotation and preparation, while Colab T4 instances are utilized for fine-tuning processes, demonstrating a structured and iterative development workflow [4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nOrganizations in education, video communications, research, and medicine are leveraging Llama 3 to create localized educational content, summarize video calls, and provide medical information in low-resource settings [5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis wide range of applications highlights the model's versatility and its potential to address real-world challenges.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe Llama 2 models overcame the tradeoff between safety and helpfulness by training two distinct reward models: the Helpfulness RM and the Safety RM\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese models optimize for different criteria, thereby achieving superior performance in both areas\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe architecture and hyper-parameters remain consistent with previous models, with adjustments made to the classification head for next-token prediction, which is replaced with a regression head for generating scalar rewards [6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis new tokenizer contributes to substantial performance gains, differentiating Llama 3 from its predecessors and competitors.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis dataset includes four times more code and over 5% high-quality, non-English data covering more than 30 languages[8]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThese enhancements in data volume and diversity are central to the model's improved performance.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta also developed a new advanced training stack to maximize GPU uptime, automating error detection, handling, and maintenance[8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe grouped query attention mechanism, which was present in Llama 2, has been further optimized for the larger models in Llama 3, contributing to its performance enhancements[7].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nDespite the foundational architecture remaining largely unchanged from LLaMA-1 models, the use of 40% more data for training underpins the significant advancements observed in Llama 3[9].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIt shows continuous log-linear improvement after training on up to 15 trillion tokens, surpassing the performance of models trained on significantly less data, such as an 8 billion parameter model with a Chinchilla-optimal training compute corresponding to approximately 200 billion tokens[8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nLlama 3's 70B model, in particular, showcases the best overall performance score, matching that of the most powerful proprietary models such as Gemini Pro 1.5 and Claude 3 Sonnet[10].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFirst, the pretraining process involved utilizing seven times more data than its predecessor, Llama 2, and careful curation of instruction-tuning data to enhance alignment and output quality[11]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, both the 8B and 70B models employ Grouped-Query Attention (GQA) for improved inference scalability, further contributing to their robust performance[12].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEvaluations conducted exclusively on 70B models revealed that the Llama 3 Chat model outperforms other open-source models by a significant margin (60â€“75%) on both single-turn and multi-turn prompts, and is comparable to ChatGPT[6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFuture releases may include a 34B parameter model upon meeting safety targets[9].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIts advanced capabilities in text generation, conversation, summarization, and translation make it a valuable tool for numerous industries.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model's ability to generate coherent and contextually appropriate text makes it suitable for tasks such as writing articles, crafting stories, and creating marketing content\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, Llama 3 excels in translation, offering robust support for multiple languages, which facilitates global communication and content localization[2][13].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis integration capability makes it a useful component in a unified and collaborative environment, improving efficiency and productivity.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis optimization makes advanced AI features more accessible on mobile devices, broadening the scope of Llama 3's applications[15].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor example, in the healthcare sector, the model can be fine-tuned using domain-specific datasets, such as medical records, to enhance its proficiency in medical Q&A applications[16]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis adaptability ensures that Llama 3 can provide accurate and contextually relevant information across various professional fields.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAdditionally, the model is used to summarize video calls, making it a useful tool for educational and research purposes[5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe capability to understand and process extensive text up to 2,048 words long allows Llama 3 to handle complex ideas and arguments, facilitating deeper insights and understanding[2].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis approach accelerates innovation and ensures that the benefits of AI advancements are widely distributed\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMeta's commitment to ethical AI development and fostering community collaboration suggests a future roadmap focused on both technological breakthroughs and user-friendliness[1].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis occurs when the model performs exceptionally well on training data but fails to generalize effectively to unseen data, potentially limiting its robustness and applicability across diverse tasks and datasets[17].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring that the model engages with content ethically and avoids harmful actions is a substantial challenge\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBalancing technological advancements with ethical responsibilities is crucial for harnessing AI's full potential for societal betterment[18]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFurthermore, in fields like medical information, the stakes for accuracy and reliability are even higher, as misinformation can lead to significant real-world health risks[4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAlthough improvements in hardware reliability and scalable storage systems have enhanced training efficiencies, the computational cost remains a barrier for widespread adoption and experimentation by smaller organizations and individual researchers[8][16].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nWhile Llama 3 has been trained on a significantly larger dataset compared to its predecessors, the curation of this data poses a substantial challenge\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring that the data is diverse and representative without including redundant or low-quality information is essential for maintaining and improving the model's performance[3]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nMissteps in data curation could lead to biased or incorrect model outputs, which are especially problematic in sensitive applications like medical Q&A[4].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nTraditional metrics like perplexity might not fully capture a model's performance, especially as the risk of inadvertently including portions of test sets in the training data increases with larger corpora[17].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe model has been praised for its significant advancements in efficiency and capability over its predecessors\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nLlama 3 has demonstrated improved attention mechanisms and alignment, resulting in a more nuanced understanding and generation of responses\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnhancements in reasoning and code generation are also notable, contributing to a diverse range of model responses and reduced false refusal rates [8].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis performance has been bolstered by rigorous evaluation methods, including the use of a 7-point Likert scale for answer quality assessment and the calculation of inter-rater reliability (IRR) to ensure consistency [6].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nFor instance, the model has been used to localize educational content for students, summarize video communications, and provide medical information in low-resource settings [5]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThe flexibility and power of Llama 3 have also been highlighted in its use for enhancing financial forecasting, healthcare analytics, and retail optimization through predictive modeling and user-friendly interfaces [19].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis competition is viewed as beneficial for the broader AI community, driving innovation and efficiency across the field [14].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBy making the model available under an open license, Meta has enabled a broader community to contribute and build upon its advancements, ensuring that the benefits of AI are widely distributed [6][1]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nIntegrations with content safety features, such as those available through Azure AI Content Safety, further ensure that the model's applications adhere to responsible AI practices [20].\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nOne of the key aspects of Llama 3's future is its open-source nature, which not only accelerates innovation by enabling a broader community to contribute and build upon the models but also ensures that the benefits of AI advancements are widely distributed[1]\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis open-source strategy reflects Meta's commitment to fostering community and collaboration, which is as crucial as the technological breakthroughs themselves.\\n\\nPlease return the question in the format: <question>\"}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEnsuring these models engage with complex content ethically while avoiding harmful actions is a significant challenge[18]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nBalancing these ethical responsibilities with technological advancements will be crucial in harnessing the full potential of AI for societal betterment\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nEfficient data management will also play a critical role in this regard, as it is essential for training, fine-tuning, and leveraging large language models effectively[18].\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nAlongside Llama 3, Meta is integrating virtual assistant features into platforms like Facebook and WhatsApp, which are based on the Llama 3 model[9]\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis integration exemplifies the broader applicability and utility of Llama 3 across various services and platforms.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': 'Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis structured, iterative development approach ensures continuous improvement and adaptation of Llama 3 for specific use cases, such as medical Q&A, through systematic evaluation and refinement.\\n\\nPlease return the question in the format: <question>'}]}\n",
      "{'model': 'gpt-4o', 'messages': [{'role': 'user', 'content': \"Please write a question which prompts the respondent to evaluate this sentence based on the wikipedia guidelines: \\n\\nThis superior performance on benchmarks like the Massive Multitask Language Understanding (MMLU) highlights Llama 3's capability to understand and perform a wide range of tasks effectively.\\n\\nPlease return the question in the format: <question>\"}]}\n"
     ]
    }
   ],
   "source": [
    "questions = [make_exam_question(sentence) for sentence in data]\n",
    "question_guidelines = [make_question_guidelines(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookup:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model_identifier = \"lookup\"\n",
    "        \n",
    "    def prompt_sequence(self,questions):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Exam(questions, question_guidelines, exam_guidelines, GradeLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [d.text for d in data]\n",
    "ex = LLMTest(Lookup(content), eval_llm, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g= ex.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_df = pd.DataFrame(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-numeric symbols from a string\n",
    "def remove_non_numeric(s):\n",
    "    return re.sub(r'[^\\d.]+', '', s)\n",
    "grade_df[\"Grade\"] = grade_df[\"Notes\"].apply(lambda x: remove_non_numeric(x.split(\"\\n\\n\")[0].split(\": \")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .8\n",
    "grade_df[\"Pass\"] = grade_df[\"Grade\"].apply(lambda x: float(x) > threshold)\n",
    "grade_df[\"Color\"] = grade_df[\"Pass\"].apply(lambda x: \"yellow\" if x else \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TA</th>\n",
       "      <th>Student</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Building on the successes of its predecessors,...</td>\n",
       "      <td>Score: 0.7\\n\\nNotes: Your response partially m...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This model stands out due to its enhanced capa...</td>\n",
       "      <td>Score: 0.6\\n\\nNotes: Your response somewhat ad...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This expansive dataset includes a rich variety...</td>\n",
       "      <td>Score: 0.8\\n\\nNotes: Your response adheres to ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Meta employs a combination of manual content r...</td>\n",
       "      <td>Score: 0.75\\n\\nNotes: Your response aligns fai...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>These measures are vital for ensuring that Lla...</td>\n",
       "      <td>Score: 0.7\\n\\nNotes: While your response adher...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Efficient data management will also play a cri...</td>\n",
       "      <td>**Score: 0.8**\\n\\n**Notes:** Your response ali...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Alongside Llama 3, Meta is integrating virtual...</td>\n",
       "      <td>**Score: 0.75**\\n\\n**Notes:** Your response al...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This integration exemplifies the broader appli...</td>\n",
       "      <td>### .65\\n\\n**Notes:** Your response somewhat a...</td>\n",
       "      <td>.65</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This structured, iterative development approac...</td>\n",
       "      <td>### .7\\n\\n**Notes:** Your response aligns with...</td>\n",
       "      <td>.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This superior performance on benchmarks like t...</td>\n",
       "      <td>### .65\\n\\n**Notes:** Your response somewhat a...</td>\n",
       "      <td>.65</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TA Student         Prompt  \\\n",
       "0   gpt-4o  lookup  body sections   \n",
       "1   gpt-4o  lookup  body sections   \n",
       "2   gpt-4o  lookup  body sections   \n",
       "3   gpt-4o  lookup  body sections   \n",
       "4   gpt-4o  lookup  body sections   \n",
       "..     ...     ...            ...   \n",
       "70  gpt-4o  lookup  body sections   \n",
       "71  gpt-4o  lookup  body sections   \n",
       "72  gpt-4o  lookup  body sections   \n",
       "73  gpt-4o  lookup  body sections   \n",
       "74  gpt-4o  lookup  body sections   \n",
       "\n",
       "                                             Response  \\\n",
       "0   Building on the successes of its predecessors,...   \n",
       "1   This model stands out due to its enhanced capa...   \n",
       "2   This expansive dataset includes a rich variety...   \n",
       "3   Meta employs a combination of manual content r...   \n",
       "4   These measures are vital for ensuring that Lla...   \n",
       "..                                                ...   \n",
       "70  Efficient data management will also play a cri...   \n",
       "71  Alongside Llama 3, Meta is integrating virtual...   \n",
       "72  This integration exemplifies the broader appli...   \n",
       "73  This structured, iterative development approac...   \n",
       "74  This superior performance on benchmarks like t...   \n",
       "\n",
       "                                                Notes Grade   Pass Color  \n",
       "0   Score: 0.7\\n\\nNotes: Your response partially m...   0.7  False   red  \n",
       "1   Score: 0.6\\n\\nNotes: Your response somewhat ad...   0.6  False   red  \n",
       "2   Score: 0.8\\n\\nNotes: Your response adheres to ...   0.8  False   red  \n",
       "3   Score: 0.75\\n\\nNotes: Your response aligns fai...  0.75  False   red  \n",
       "4   Score: 0.7\\n\\nNotes: While your response adher...   0.7  False   red  \n",
       "..                                                ...   ...    ...   ...  \n",
       "70  **Score: 0.8**\\n\\n**Notes:** Your response ali...   0.8  False   red  \n",
       "71  **Score: 0.75**\\n\\n**Notes:** Your response al...  0.75  False   red  \n",
       "72  ### .65\\n\\n**Notes:** Your response somewhat a...   .65  False   red  \n",
       "73  ### .7\\n\\n**Notes:** Your response aligns with...    .7  False   red  \n",
       "74  ### .65\\n\\n**Notes:** Your response somewhat a...   .65  False   red  \n",
       "\n",
       "[75 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge grade df with df on index\n",
    "df = df.merge(grade_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>section_type</th>\n",
       "      <th>subsection</th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>claim_number</th>\n",
       "      <th>text</th>\n",
       "      <th>TA</th>\n",
       "      <th>Student</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Building on the successes of its predecessors,...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Building on the successes of its predecessors,...</td>\n",
       "      <td>Score: 0.7\\n\\nNotes: Your response partially m...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This model stands out due to its enhanced capa...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This model stands out due to its enhanced capa...</td>\n",
       "      <td>Score: 0.6\\n\\nNotes: Your response somewhat ad...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This expansive dataset includes a rich variety...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This expansive dataset includes a rich variety...</td>\n",
       "      <td>Score: 0.8\\n\\nNotes: Your response adheres to ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Meta employs a combination of manual content r...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Meta employs a combination of manual content r...</td>\n",
       "      <td>Score: 0.75\\n\\nNotes: Your response aligns fai...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>These measures are vital for ensuring that Lla...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>These measures are vital for ensuring that Lla...</td>\n",
       "      <td>Score: 0.7\\n\\nNotes: While your response adher...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Efficient data management will also play a cri...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Efficient data management will also play a cri...</td>\n",
       "      <td>**Score: 0.8**\\n\\n**Notes:** Your response ali...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Alongside Llama 3, Meta is integrating virtual...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Alongside Llama 3, Meta is integrating virtual...</td>\n",
       "      <td>**Score: 0.75**\\n\\n**Notes:** Your response al...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>This integration exemplifies the broader appli...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This integration exemplifies the broader appli...</td>\n",
       "      <td>### .65\\n\\n**Notes:** Your response somewhat a...</td>\n",
       "      <td>.65</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>This structured, iterative development approac...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This structured, iterative development approac...</td>\n",
       "      <td>### .7\\n\\n**Notes:** Your response aligns with...</td>\n",
       "      <td>.7</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Future Prospects</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>This superior performance on benchmarks like t...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This superior performance on benchmarks like t...</td>\n",
       "      <td>### .65\\n\\n**Notes:** Your response somewhat a...</td>\n",
       "      <td>.65</td>\n",
       "      <td>False</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             section   section_type        subsection  paragraph_number  \\\n",
       "0          # summary   Lead section         # summary                 2   \n",
       "1          # summary   Lead section         # summary                 2   \n",
       "2          # summary   Lead section         # summary                 2   \n",
       "3          # summary   Lead section         # summary                 3   \n",
       "4          # summary   Lead section         # summary                 3   \n",
       "..               ...            ...               ...               ...   \n",
       "70  Future Prospects  body sections  Future Prospects                 3   \n",
       "71  Future Prospects  body sections  Future Prospects                 4   \n",
       "72  Future Prospects  body sections  Future Prospects                 4   \n",
       "73  Future Prospects  body sections  Future Prospects                 5   \n",
       "74  Future Prospects  body sections  Future Prospects                 6   \n",
       "\n",
       "    claim_number                                               text      TA  \\\n",
       "0              1  Building on the successes of its predecessors,...  gpt-4o   \n",
       "1              2  This model stands out due to its enhanced capa...  gpt-4o   \n",
       "2              3  This expansive dataset includes a rich variety...  gpt-4o   \n",
       "3              1  Meta employs a combination of manual content r...  gpt-4o   \n",
       "4              2  These measures are vital for ensuring that Lla...  gpt-4o   \n",
       "..           ...                                                ...     ...   \n",
       "70             3  Efficient data management will also play a cri...  gpt-4o   \n",
       "71             1  Alongside Llama 3, Meta is integrating virtual...  gpt-4o   \n",
       "72             2  This integration exemplifies the broader appli...  gpt-4o   \n",
       "73             1  This structured, iterative development approac...  gpt-4o   \n",
       "74             1  This superior performance on benchmarks like t...  gpt-4o   \n",
       "\n",
       "   Student         Prompt                                           Response  \\\n",
       "0   lookup  body sections  Building on the successes of its predecessors,...   \n",
       "1   lookup  body sections  This model stands out due to its enhanced capa...   \n",
       "2   lookup  body sections  This expansive dataset includes a rich variety...   \n",
       "3   lookup  body sections  Meta employs a combination of manual content r...   \n",
       "4   lookup  body sections  These measures are vital for ensuring that Lla...   \n",
       "..     ...            ...                                                ...   \n",
       "70  lookup  body sections  Efficient data management will also play a cri...   \n",
       "71  lookup  body sections  Alongside Llama 3, Meta is integrating virtual...   \n",
       "72  lookup  body sections  This integration exemplifies the broader appli...   \n",
       "73  lookup  body sections  This structured, iterative development approac...   \n",
       "74  lookup  body sections  This superior performance on benchmarks like t...   \n",
       "\n",
       "                                                Notes Grade   Pass Color  \n",
       "0   Score: 0.7\\n\\nNotes: Your response partially m...   0.7  False   red  \n",
       "1   Score: 0.6\\n\\nNotes: Your response somewhat ad...   0.6  False   red  \n",
       "2   Score: 0.8\\n\\nNotes: Your response adheres to ...   0.8  False   red  \n",
       "3   Score: 0.75\\n\\nNotes: Your response aligns fai...  0.75  False   red  \n",
       "4   Score: 0.7\\n\\nNotes: While your response adher...   0.7  False   red  \n",
       "..                                                ...   ...    ...   ...  \n",
       "70  **Score: 0.8**\\n\\n**Notes:** Your response ali...   0.8  False   red  \n",
       "71  **Score: 0.75**\\n\\n**Notes:** Your response al...  0.75  False   red  \n",
       "72  ### .65\\n\\n**Notes:** Your response somewhat a...   .65  False   red  \n",
       "73  ### .7\\n\\n**Notes:** Your response aligns with...    .7  False   red  \n",
       "74  ### .65\\n\\n**Notes:** Your response somewhat a...   .65  False   red  \n",
       "\n",
       "[75 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>section_type</th>\n",
       "      <th>subsection</th>\n",
       "      <th>paragraph_number</th>\n",
       "      <th>claim_number</th>\n",
       "      <th>text</th>\n",
       "      <th>TA</th>\n",
       "      <th>Student</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td># summary</td>\n",
       "      <td>Lead section</td>\n",
       "      <td># summary</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Issues such as overfitting, ethical concerns, ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Issues such as overfitting, ethical concerns, ...</td>\n",
       "      <td>Score: 0.85\\n\\nNotes: Your response is quite w...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Limitations</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Overfitting and Data Dependence</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>This occurs when the model performs exceptiona...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>This occurs when the model performs exceptiona...</td>\n",
       "      <td>**Score: 0.85**\\n\\n**Notes:** Your response la...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Limitations</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Computational Resources and Efficiency</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Although improvements in hardware reliability ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Although improvements in hardware reliability ...</td>\n",
       "      <td>**Score: 0.85**\\n\\n**Notes:** Your response la...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Limitations</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Dataset Quality and Curation</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Missteps in data curation could lead to biased...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Missteps in data curation could lead to biased...</td>\n",
       "      <td>**Score: 0.85**\\n\\n**Notes:** Your response ad...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Limitations</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Evaluation Challenges</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Traditional metrics like perplexity might not ...</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>lookup</td>\n",
       "      <td>body sections</td>\n",
       "      <td>Traditional metrics like perplexity might not ...</td>\n",
       "      <td>**Score: 0.85**\\n\\n**Notes:** Your response ad...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section   section_type                              subsection  \\\n",
       "9     # summary   Lead section                               # summary   \n",
       "48  Limitations  body sections         Overfitting and Data Dependence   \n",
       "52  Limitations  body sections  Computational Resources and Efficiency   \n",
       "55  Limitations  body sections            Dataset Quality and Curation   \n",
       "56  Limitations  body sections                   Evaluation Challenges   \n",
       "\n",
       "    paragraph_number  claim_number  \\\n",
       "9                  5             1   \n",
       "48                 2             1   \n",
       "52                 2             1   \n",
       "55                 2             3   \n",
       "56                 2             1   \n",
       "\n",
       "                                                 text      TA Student  \\\n",
       "9   Issues such as overfitting, ethical concerns, ...  gpt-4o  lookup   \n",
       "48  This occurs when the model performs exceptiona...  gpt-4o  lookup   \n",
       "52  Although improvements in hardware reliability ...  gpt-4o  lookup   \n",
       "55  Missteps in data curation could lead to biased...  gpt-4o  lookup   \n",
       "56  Traditional metrics like perplexity might not ...  gpt-4o  lookup   \n",
       "\n",
       "           Prompt                                           Response  \\\n",
       "9   body sections  Issues such as overfitting, ethical concerns, ...   \n",
       "48  body sections  This occurs when the model performs exceptiona...   \n",
       "52  body sections  Although improvements in hardware reliability ...   \n",
       "55  body sections  Missteps in data curation could lead to biased...   \n",
       "56  body sections  Traditional metrics like perplexity might not ...   \n",
       "\n",
       "                                                Notes Grade  Pass   Color  \n",
       "9   Score: 0.85\\n\\nNotes: Your response is quite w...  0.85  True  yellow  \n",
       "48  **Score: 0.85**\\n\\n**Notes:** Your response la...  0.85  True  yellow  \n",
       "52  **Score: 0.85**\\n\\n**Notes:** Your response la...  0.85  True  yellow  \n",
       "55  **Score: 0.85**\\n\\n**Notes:** Your response ad...  0.85  True  yellow  \n",
       "56  **Score: 0.85**\\n\\n**Notes:** Your response ad...  0.85  True  yellow  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Color==\"yellow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"outputs/Llama_3_(Language_Model)//graded_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
